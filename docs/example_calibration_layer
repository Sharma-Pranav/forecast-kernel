
## ğŸ§  **FIL-âˆ Architecture â€” Live + Dev Modes with Diagnostic Calibration **

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚         Raw Time Series Data Sources       â”‚
        â”‚  (Sales, Orders, Price, Promo, Calendar)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚        Data Preprocessing & Feature Gen    â”‚
        â”‚  - Calendar enrichment                     â”‚
        â”‚  - Lag/rolling stats                       â”‚
        â”‚  - Promo/price/stockout handling           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Scenario Injection Engine  â”‚   â—„â”€â”€ Manual or scripted
        â”‚  (stockout, cold start, etc) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           Forecast Model Layer             â”‚
        â”‚  - TiRex (xLSTM)     â† internal benchmark  â”‚
        â”‚  - TIME-MoE          â† production-grade    â”‚
        â”‚  - Darts / NeuralForecast (SOTA baselines) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ğŸ“¦ Live Forecast Path    â”‚  â”‚   ğŸ§ª Development & Calibration Path (via `toto`)        â”‚
â”‚ - No y_true available        â”‚  â”‚ - For backtesting, diagnostics, and model tuning       â”‚
â”‚ - Used in operational flows  â”‚  â”‚ - Evaluate & recalibrate forecast reliability          â”‚
â”‚                              â”‚  â”‚ - Visualize sharpness & coverage                       â”‚
â”‚                              â”‚  â”‚ - Train `Calibrator()` and export to production        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚                                               â–²
                             â–¼                                               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   ğŸ› ï¸  (Optional) Calibrator Inference Layer  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ Apply `Calibrator.transform` â”‚
        â”‚ - Load pretrained calibrator object         â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ - Correct quantile forecasts before output  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚      Evaluation & Diagnostic Metrics        â”‚
        â”‚  - MAE, Bias, SMAPE                         â”‚
        â”‚  - Score = MAE + |Bias|                     â”‚
        â”‚  - ABC-XYZ Class Accuracy                   â”‚
        â”‚  - Forecast Value Added (FVA)               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚       Forecast Output + Metadata Layer      â”‚
        â”‚  - Versioning (DVC, MLflow)                 â”‚
        â”‚  - Export to Obsidian, CSV, Gamma slides    â”‚
        â”‚  - For decks, audits, client trust layers   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§ª `toto` Calibration Code (Used in Development Layer Only)

```python
from toto.metrics import coverage, sharpness, reliability_curve
from toto.calibration import Calibrator
from toto.plotting import plot_reliability_diagram

# Inputs:
# y_preds: array of quantile forecasts, shape [n_samples, n_quantiles]
# y_true: actual target values
# quantiles: list of quantile levels (e.g., [0.1, 0.5, 0.9])

# Step 1: Evaluate forecast calibration
coverage_scores = coverage(y_preds, y_true, quantiles)
sharpness_score = sharpness(y_preds)
rel_x, rel_y = reliability_curve(y_preds, y_true, quantiles)

print("Coverage:", dict(zip(quantiles, coverage_scores)))
print("Sharpness Score:", sharpness_score)

# Optional: Visualize reliability diagram
plot_reliability_diagram(rel_x, rel_y, quantiles)

# Step 2: Recalibrate forecast quantiles
calibrator = Calibrator()
calibrator.fit(y_preds, y_true, quantiles)
y_recalibrated = calibrator.transform(y_preds, quantiles)
```

---

## ğŸ” Productionizing the Calibrator

Once validated offline, the `Calibrator` can be moved to live production:

### âœ… Save Once in Dev

```python
import joblib
joblib.dump(calibrator, "quantile_calibrator.pkl")
```

### âœ… Apply in Production

```python
calibrator = joblib.load("quantile_calibrator.pkl")
calibrated_preds = calibrator.transform(y_preds_live, quantiles)
```

ğŸ’¡ *This enables real-time correction of quantile forecasts â€” trust alignment without retraining.*

---

## ğŸ“Œ Embedded Design Intelligence & Execution Notes

### âš  Assumptions

* Forecast model must support **quantile outputs**
* `toto` calibration logic is only valid when **ground truth (`y_true`) is available**
* Quantiles must be consistent between calibration and inference phases

---

### âœ… When to Use `toto`

| Use Case                      | Use `toto`?                             |
| ----------------------------- | --------------------------------------- |
| Live forecasts                | âœ… After calibrator is trained and saved |
| Backtesting loop              | âœ… Yes                                   |
| Quarterly Codex reviews       | âœ… Yes                                   |
| Forecast Value Added analysis | âœ… Yes                                   |
| Client deck or dashboard      | âœ… Yes (plots, insights)                 |

---

### ğŸ” Integration Suggestion (Hydra-style)

```yaml
mode: dev
forecast:
  quantiles: [0.1, 0.5, 0.9]
  use_calibration: true
```

Then:

```python
if config.mode == "dev" and config.forecast.use_calibration:
    run_toto_evaluation(...)
```

---

### ğŸ’¾ Log This in MLflow/DVC

For each run:

* Save `coverage_scores`, `sharpness_score`, and `recalibrator.pkl`
* Export reliability diagram `.png`
* Tag: `model=TIME-MoE_v1.2-calibrated`, `calibrator=p10-p90-v3`

---

## ğŸ§  Why This Matters

| Layer                    | Role                                       | Output                      |
| ------------------------ | ------------------------------------------ | --------------------------- |
| **Forecast Model Layer** | Generates quantile predictions             | `y_preds`                   |
| **Calibration (toto)**   | Validates and trains post-hoc correction   | Calibration scores          |
| **Recalibration (prod)** | Corrects quantile spread at inference time | `y_calibrated`              |
| **Evaluation Layer**     | Final trust metrics + FVA                  | MAE, Bias, Score            |
| **Codex Layer**          | Feeds forecasts + trust metrics into tools | Strategy-grade intelligence |

---

